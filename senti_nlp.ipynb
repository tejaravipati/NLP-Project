{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "senti_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUqFFqcNjXen",
        "outputId": "53f1e462-e43a-410f-d63c-31b9e472b902"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPhZKaQoio_z"
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from bs4 import BeautifulSoup\n",
        "import argparse\n",
        "import cPickle\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "doc = nlp(u\"I don't want parsed\", disable=['parser','tag','entity'])\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list.remove('not')\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz4JYhfEkCBj"
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBgpzutikB9P"
      },
      "source": [
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjhmwHUhkB7A"
      },
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7N6zDkgkB4x"
      },
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvH5kRytkB2h"
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    for doc in corpus:\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)  \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)   \n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)   \n",
        "        if special_char_removal:\n",
        "            doc = remove_special_characters(doc)  \n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)    \n",
        "        normalized_corpus.append(doc)  \n",
        "    return normalized_corpus"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF9bSwZakB0c"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files='IMDB-Dataset.csv')\n",
        "base_url = 'https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/'\n",
        "dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t_y2koikByS"
      },
      "source": [
        "def random_forest_on_bow(dataset, extend_data=False, vocab_limit=5000, forest_size=100):\n",
        "    assert dataset in ['IMDB-Dataset.csv']\n",
        "    print \"Loading\"\n",
        "    remove_stop = True\n",
        "    extract_tokens = False\n",
        "    if dataset == 'imdb':\n",
        "        data_dir = IMDB_DATA_DIR\n",
        "    else:\n",
        "        print \"ERROR: Unknown dataset %s\" % dataset\n",
        "        return\n",
        "    data_file_name = data_dir + \"/processed/data_extended%s_remove-stop%s_tokens%s.pkl\" % (\n",
        "        (\"1\" if extend_data else \"0\"),\n",
        "        (\"1\" if remove_stop else \"0\"),\n",
        "        (\"1\" if extract_tokens else \"0\")\n",
        "    )\n",
        "    data = cPickle.load(open(data_file_name, 'rb'))\n",
        "\n",
        "    labeled_train, unlabeled_train, test = data[0], data[1], data[2]\n",
        "    labeled_train_reviews = data[3] \n",
        "    unlabeled_train_reviews = data[4]  \n",
        "    test_reviews = data[5] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zWhcnmikBvv"
      },
      "source": [
        "    print(\"bag of words\")\n",
        "    vectorizer = CountVectorizer(max_features=vocab_limit)\n",
        "    train_data_features = vectorizer.fit_transform(labeled_train_reviews)\n",
        "    train_data_features = train_data_features.toarray()\n",
        "    print train_data_features\n",
        "    print train_data_features.shape\n",
        "\n",
        "    vocab = vectorizer.get_feature_names()\n",
        "    print(\"Vocabulary size: %d\" % len(vocab))\n",
        "\n",
        "    dist = np.sum(train_data_features, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SExQOSekBsD"
      },
      "source": [
        "    print(\"Training Random Forest(%d) classifier\" % forest_size)\n",
        "    forest = RandomForestClassifier(n_estimators=forest_size) \n",
        "    if dataset == 'IMDB-Dataset.csv':\n",
        "        forest = forest.fit(train_data_features, labeled_train['train'])\n",
        "    else:\n",
        "        print( \"ERROR: Unknown dataset %s\" % dataset)\n",
        "        return\n",
        "    print (\"done.\")\n",
        "\n",
        "    train_predictions = forest.predict(train_data_features)\n",
        "    if dataset == 'imdb':\n",
        "        mse = mean_squared_error(labeled_train['train'], train_predictions)\n",
        "    else:\n",
        "        print (\"ERROR: Unknown dataset %s\" % dataset)\n",
        "        return\n",
        "    print (\"TRAINING model =\", model_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQNzBVFkBjd"
      },
      "source": [
        "    print (\"Fitting test data\")\n",
        "    test_data_features = vectorizer.transform(test_reviews)\n",
        "    test_data_features = test_data_features.toarray()\n",
        "    print test_data_features\n",
        "    print test_data_features.shape\n",
        "\n",
        "    print(\"Making predictions on test data\")\n",
        "    test_predictions = forest.predict(test_data_features)\n",
        "    print \"done.\"\n",
        "\n",
        "    print(\"Writting predictions to file\")\n",
        "    if dataset == 'IMDB-Dataset.csv':\n",
        "        output = pd.DataFrame(data={\"id\": test['id'], \"train\": test_predictions})\n",
        "    else:\n",
        "        print(\"ERROR: Unknown dataset %s\" % dataset)\n",
        "        return\n",
        "    file_name = \"./ModelResponses/%s_forest%d_bow%d_%spredictions.csv\"\\\n",
        "                % (dataset, forest_size, vocab_limit, (\"extended_\" if extend_data else \"\"))\n",
        "    output.to_csv(file_name, index=False, quoting=3)\n",
        "    print(\"done.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWHddjWTkBbA"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.register('type', 'bool', lambda v: v.lower() in (\"yes\", \"true\", \"t\", \"1\"))\n",
        "    parser.add_argument(\n",
        "        dataset = 'IMDB-Dataset.csv',\n",
        "        help='Dataset to load and predict on.'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--features_size',\n",
        "        type=int,\n",
        "        default=10000,\n",
        "        help='Number of features per review = vocab size.'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--forest_size',\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help='Number of classification trees in Random Forest algorithm.'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--extend',\n",
        "        type='bool',\n",
        "        default=False,\n",
        "        help='Flag to decide to load the extended data ('IMDB-Dataset.csv') or not.'\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    print('args:', args)\n",
        "\n",
        "    print(\"Running RandomForest on BagOfWords features\")\n",
        "    random_forest_on_bow(args.dataset, extend_data=args.extend, vocab_limit=args.features_size, forest_size=args.forest_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTgq1qlYkBLR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}